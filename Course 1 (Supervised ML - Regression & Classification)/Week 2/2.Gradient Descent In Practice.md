# Feature Scaling:
## We need feature scaling
- When a possible range of values of a feature is large, is likely that a good model will learn to choose a relatively small parameter value of `w` and The opposite like that
![Feature and parameter value](/Images/C1_W2_FeatureAndParameterValues.png)  
*Figure: Showing the the feature and parameter values*

- So by that when there like a high range of values in a specified feature that the others in our training data, that will likely cause the gradient descent not performing well
as the shape will not be a perfect counter plot so the `GD` will bouncing back and force long time before it reaches the global minimum
- In situations like this a good thing to do is to scale the features to make `GD` perform well and quick.
- Scaling is like to make one range of values for all the input features.
![Feature size and gradient descent](/Images/C1_W2_FeatureSize&GradientDescent.png)  
*Figure: Showing the impact of the feature size on gradient descent*
## How to make feature scaling
- There is a couple of ways to make scaling like *(Deviding by the maximum value, Mean Normalization, Z-score normalization)*
### Mean Normalization
- It is to take the average of a giving feature then subtract this from the input feature the devide by the diff between the greatest and min value
- the mean normalization is calculated like in the following figure:
![Mean Normalization](/Images/C1_W2_MeanNormalization.png)  
*Figure: Showing the impact of the feature size on gradient descent*
### Z-Score Normalization:
- To calculate it we need to take the standard deviation and mean of each feature
- Subtract the mean from each feature and devide by the standard deviation.
![Mean Normalization](/Images/C1_W2_Z-ScoreNormalization.png)  
*Figure: Showing the impact of the feature size on gradient descent*  
## Rescaling
- Scale again if the range of input features is too large or too low, explained in the figure below:
![Feature Rescaling](/Images/C1_W2_FeatureRescaling.png)  
*Figure: Showing the conditions that is necessary to rescale the features*
# Checking gradient descent for convergence
- If the cost function after each iteration of the `GD` decreases then it is working properly.
- If the cost function ever increases after one iteration, that means eihther *alpha is chosen poorly(usually alpha is too large), or there could be bug in the code*.
- When the cost funtion values is the same or it doesnt change too much then the `GD` has converged.
- The `GD` conversion varies between differenct applications, for some application it could be after like 30 iterations, and for another it could be after like 100,000 iteration.
- It is very difficult to tell in advance how many iterations `GD` needs to converge, so we create a graph like in the figure to try to findout where you can start training your particular model.

**Automatic Convergence Test**
- Measuring the cost function with the `epsilon` value , it is very small, so after one iteration if the cost function value is decreasing with a value minimum than epsilon then declare *convergence*.  
- choosing the right value of epsilon is very difficult, so usually you can look at the *Learning rate graph to find where the model start to converge*.
![Learning Rate Graph](/Images/C1_W2_LearningRateGraph.png)  
*Figure: Showing the learning rate curve to figure where the `GD` start converging*
# Choosing the learning rate
- Choosing the learning rate affects the algorithm speed, when it is too small it may converge slowly, if it too hight it may not converge.
- When we choosing a very small alph, the J(w,b) should decrease.
- To identify if the `GD` is working properly is to choose a very small alpha, and if the cost is decreasing after each iteration, then it is working properly, if not there likely a bug in the code.
- Notice that choosing a very small value of alpha is for debugging, as it is not the best effecient choice of training the algorithm as it will take very large iterations to converge.
- Usually we begin with a `0.001` and then try 10 times larger or less and so on, then plot the cost with the iterations, and after some iterations pick the value of alpha that seems to decrease the learning rate properly *(iterations vs cost graph)* 
![Learning Rate Problems](/Images/C1_W2_learningRateProblem.png)  
*Figure: Showing the possbile problems when choosing the learning rate*
---
![Learning Rate Problems](/Images/C1_W2_ChoosingAlpha.png)  
*Figure: Showing how to choose the suitable alpha value*
# Feature Engineering
# Polynomial regression
# 