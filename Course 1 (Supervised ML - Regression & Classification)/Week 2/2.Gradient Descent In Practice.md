# Feature Scaling

## Why We Need Feature Scaling
- When a feature has a **large range of values**, the corresponding parameter \( w \) will often be **small** in magnitude to balance its effect in the model.
- If different features have **very different ranges**, gradient descent struggles:
  - The cost function contours become **elongated ellipses** instead of circles.
  - Gradient descent "zig-zags" and takes longer to reach the minimum.
- Scaling features so they have similar ranges makes gradient descent **faster and more efficient**.

![Feature and parameter value](/Images/C1_W2_FeatureAndParameterValues.png)  
*Figure: Relationship between feature ranges and parameter magnitudes.*

![Feature size and gradient descent](/Images/C1_W2_FeatureSize&GradientDescent.png)  
*Figure: Large feature ranges cause inefficient gradient descent paths.*

---

## How to Perform Feature Scaling

### Mean Normalization
- Formula:
  ```
  x_j[i]_scaled = ( x_j[i] - mu_j ) / ( max(x_j) - min(x_j) )
  ```
  where:
  - `mu_j` = mean of feature `j`
  - `max(x_j)`, `min(x_j)` = maximum and minimum of feature `j`

![Mean Normalization](/Images/C1_W2_MeanNormalization.png)  
*Figure: Mean normalization formula and effect.*

---

### Z-Score Normalization
- Formula:
  ```
  x_j[i]_scaled = ( x_j[i] - mu_j ) / sigma_j
  ```
  where:
  - `sigma_j` = standard deviation of feature `j`
  - Centers features around 0 and scales by their variability.

![Z-Score Normalization](/Images/C1_W2_Z-ScoreNormalization.png)  
*Figure: Z-score scaling process.*
---

### Rescaling Considerations
- If after initial scaling the feature ranges are **still too large or too small**, rescale again.
- This ensures **balanced feature influence**.

![Feature Rescaling](/Images/C1_W2_FeatureRescaling.png)  
*Figure: When to rescale features.*

---

# Checking Gradient Descent for Convergence

## Signs Gradient Descent is Working Properly
- **Cost decreases** after each iteration → good.
- **Cost increases** at any step → likely:
  - Learning rate (\( \alpha \)) is too large.
  - There’s a bug in the code.

## Convergence
- If cost changes very little between iterations → gradient descent has converged.
- The required number of iterations depends on the problem:
  - Could be **tens** for simple problems.
  - Could be **hundreds of thousands** for complex datasets.

![Learning Rate Graph](/Images/C1_W2_LearningRateGraph.png)  
*Figure: Cost vs. iterations for checking convergence.*

---

## Automatic Convergence Test
- Use a small **epsilon** threshold:
  - If the cost decreases by **less than epsilon** between iterations → stop.
- Choosing epsilon is tricky, so most people **visually check** the cost graph.

---

# Choosing the Learning Rate (\( \alpha \))

## Effects of \( \alpha \)
- Too small → slow convergence.
- Too large → may not converge or may diverge.

## Debugging with \( \alpha \)
- Pick a **very small value** (e.g., \( 0.0001 \)) to verify correctness.
- If cost decreases every iteration → code likely correct.
- Then increase \( \alpha \) systematically (e.g., ×10 each time) to find an efficient value.

## Practical Approach
1. Start with \( \alpha = 0.001 \).
2. Try ×10 larger and ×10 smaller values.
3. Plot cost vs. iterations.
4. Choose the largest \( \alpha \) that still decreases cost smoothly.

![Learning Rate Problems](/Images/C1_W2_learningRateProblem.png)  
*Figure: Problems from choosing \( \alpha \) too high or too low.*

![Choosing Alpha](/Images/C1_W2_ChoosingAlpha.png)  
*Figure: Systematic approach to selecting \( \alpha \).*

# Feature Engineering
# Polynomial regression
# 