# Feature Scaling

## Why We Need Feature Scaling
- When a feature has a **large range of values**, the corresponding parameter \( w \) will often be **small** in magnitude to balance its effect in the model.
- If different features have **very different ranges**, gradient descent struggles:
  - The cost function contours become **elongated ellipses** instead of circles.
  - Gradient descent "zig-zags" and takes longer to reach the minimum.
- Scaling features so they have similar ranges makes gradient descent **faster and more efficient**.

![Feature and parameter value](/Images/C1_W2_FeatureAndParameterValues.png)  
*Figure: Relationship between feature ranges and parameter magnitudes.*

![Feature size and gradient descent](/Images/C1_W2_FeatureSize&GradientDescent.png)  
*Figure: Large feature ranges cause inefficient gradient descent paths.*

---

## How to Perform Feature Scaling

### Mean Normalization
- Formula:
  ```
  x_j[i]_scaled = ( x_j[i] - mu_j ) / ( max(x_j) - min(x_j) )
  ```
  where:
  - `mu_j` = mean of feature `j`
  - `max(x_j)`, `min(x_j)` = maximum and minimum of feature `j`

![Mean Normalization](/Images/C1_W2_MeanNormalization.png)  
*Figure: Mean normalization formula and effect.*

--- 

### Z-Score Normalization
- Formula:
  ```
  x_j[i]_scaled = ( x_j[i] - mu_j ) / sigma_j
  ```
  where:
  - `sigma_j` = standard deviation of feature `j`
  - Centers features around 0 and scales by their variability.

![Z-Score Normalization](/Images/C1_W2_Z-ScoreNormalization.png)  
*Figure: Z-score scaling process.*
---

### Rescaling Considerations
- If after initial scaling the feature ranges are **still too large or too small**, rescale again.
- This ensures **balanced feature influence**.

![Feature Rescaling](/Images/C1_W2_FeatureRescaling.png)  
*Figure: When to rescale features.*

---

# Checking Gradient Descent for Convergence

## Signs Gradient Descent is Working Properly
- **Cost decreases** after each iteration → good.
- **Cost increases** at any step → likely:
  - Learning rate ($\alpha$) is too large.
  - There’s a bug in the code.

## Convergence
- If cost changes very little between iterations → gradient descent has converged.
- The required number of iterations depends on the problem:
  - Could be **tens** for simple problems.
  - Could be **hundreds of thousands** for complex datasets.

![Learning Rate Graph](/Images/C1_W2_LearningRateGraph.png)  
*Figure: Cost vs. iterations for checking convergence.*

---

## Automatic Convergence Test
- Use a small **epsilon** threshold:
  - If the cost decreases by **less than epsilon** between iterations → stop.
- Choosing epsilon is tricky, so most people **visually check** the cost graph.

---

# Choosing the Learning Rate ($\alpha$)

## Effects of ($\alpha$)
- Too small → slow convergence.
- Too large → may not converge or may diverge.

## Debugging with ($\alpha$)
- Pick a **very small value** (e.g., \( 0.0001 \)) to verify correctness.
- If cost decreases every iteration → code likely correct.
- Then increase ($\alpha$) systematically (e.g., ×10 each time) to find an efficient value.

## Practical Approach
1. Start with \( $\alpha$ = 0.001 \).
2. Try ×10 larger and ×10 smaller values.
3. Plot cost vs. iterations.
4. Choose the largest ($\alpha$) that still decreases cost smoothly.

![Learning Rate Problems](/Images/C1_W2_learningRateProblem.png)  
*Figure: Problems from choosing ($\alpha$) too high or too low.*

![Choosing Alpha](/Images/C1_W2_ChoosingAlpha.png)  
*Figure: Systematic approach to selecting ($\alpha$).*

# Feature Engineering

- **Feature engineering** means creating new features from the original input data to help the learning algorithm capture important patterns.  
- This can involve:
  - Transforming a feature (e.g., taking logarithms, squaring, normalizing).  
  - Combining features (e.g., multiplying two features together).  
  - Creating polynomial features (e.g., \( x, x^2, x^3, \dots \)) to allow more flexible decision boundaries.  

- In simple linear regression, the model is limited to fitting a **straight line**.  
- After feature engineering (like adding polynomial features), the model is no longer just a straight line — it can curve and adapt better to the data.  
- This naturally leads to **polynomial regression**, which is still linear regression in terms of parameters, but applied on polynomially expanded features.  

> **Key idea:** Feature engineering helps the algorithm represent the data in a more useful way, often making linear regression (or other models) perform much better.

![Feature Engineering](/Images/C1_W2_FeatureEngineering.png)  
*Figure: Showing what is feature engineering*

# Polynomial Regression
- **Polynomial regression** is just linear regression applied to polynomially expanded features.  
- By adding powers of features (x, x², x³, …) and their interactions (x1·x2, …), the model can fit curves instead of being limited to straight lines.  
- The model is still **linear in the parameters** (weights), but **nonlinear in the inputs**.  

### Why scaling matters
- Polynomial expansion creates features with very different ranges (e.g., x vs. x² vs. x³).  
- **Feature scaling** (like z-score normalization) is crucial to make gradient descent converge efficiently.  

### Choice of features
- You decide which polynomial terms or interactions to include (by choosing that fits the model curve well).  
- Higher degrees give more flexibility but risk **overfitting**.  
- Start small (degree 2 or 3) and use validation to check if extra terms help.  

![Polynomial Regression](/Images/C1_W2_PolynomialRegression.png)  
*Figure: Showing what is Polynomial Regression*  

![Choice Of Features](/Images/C1_W2_ChoiceOfFeatures.png)  
*Figure: Showing the effect of choosing features*  
