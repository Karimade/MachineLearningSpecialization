# Squared Error Cost Function For Logistic Regression
- It turns out that the `Squared Error` cost function is not suitable for logistic regression with gradient descent.
- The reason is due to that logistic regression is not linear nature and when we plot the cost function it has a `Non-Convex` shape,
and we also know that gradient descent is trying to reach he global local minum in the Convex shape,
so when we use this cost function it will reach only the first local minimum it founds and it will never reach the global minimu.

![Squared Error Cost Function for logistic regression](/Images/C1_W3_SquarredErrorCost.png)  
*Figure: Squared Error Cost Function for Rogistic Regression.*  

# Cost Function For Logisict Regression
- To make the cost function for linear regression takes a convex shape inorder to make the grandient descent converge to the global minimum
we will update the cost function like this:  
## Lose Function 
**Loss** is a measure of the difference of a single example to its target value while the  
**Cost** is a measure of the losses over the training set
- We will extract the inner part of the cost function and call it a `Loss Function`.
- And this Loss Function has two parts based on the label y like in the figure.
- The loss function is a measure of one training example.
- Because logistic regression output ranges from 0 to 1 so in the figure we will take the shape part from 0 to one.
- When `Y` is equal to one and the predicted value is equal to one the Loss Function is 0, and when it equal to 0, The loss function is infinity.
![Loss Function with Y = 1](/Images/C1_W3_LossFunction_Y1.png)   
*Figure: Loss Function when Y is equal to 1*  

- When `Y` is equal to zero and the predicted value is equal to zero the Loss Function is 0, and when it equal to 1, The loss function is infinity.
![Loss Function with Y = 0](/Images/C1_W3_LossFunction_Y0.png)  
*Figure: Loss Function when Y is equal to 0* 

- The defining feature of this loss function is the fact that it uses two separate curves. One for the case when the target is zero or ($y=0$) and another for when the target is one ($y=1$). Combined, these curves provide the behavior useful for a loss function, namely, being zero when the prediction matches the target and rapidly increasing in value as the prediction differs from the target.
- So now after we have update the cost function, now it is ready to be used for logistic regression and it will have the convex shape to make GD converge to the global minmum.
![Cost Function for Logistic Regression](/Images/C1_W3_CostFunctionForLogisticRegression.png)  
*Figure: Cost Function for Logistic Regression* 

# Simplified Loss Function For Logisitc Regression
- Instead of separating the loss function in two parts based on either y is one or zero 
we will combine them in one simplified version as:
![Simplified Loss Function for Logistic Regression](/Images/C1_W3_SimplifiedLossFunction.png)  
*Figure: Simplified Loss Function for Logistic Regression* 

# Simplified Cost Function For Logisitic Regression
- This cost function is derived from statics using a statistical princible called `Maximum Likelihood Estimation`
![Simplified Cost Function for Logistic Regression](/Images/C1_W3_SimplifiedCostFunction.png)  
*Figure: Simplified Cost Function for Logistic Regression* 

