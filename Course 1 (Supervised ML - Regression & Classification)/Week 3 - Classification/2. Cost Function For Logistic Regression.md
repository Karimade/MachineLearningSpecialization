# Squared Error Cost Function for Logistic Regression
- The **squared error cost function** (used in linear regression) is **not suitable** for logistic regression.  
- Reason: Logistic regression is **nonlinear**, so the squared error cost produces a **non-convex** function.  
- Gradient descent works best with **convex cost functions**, where it can find the **global minimum**.  
- With a non-convex cost, gradient descent may get stuck in a **local minimum**.

![Squared Error Cost Function for Logistic Regression](/Images/C1_W3_SquarredErrorCost.png)  
*Figure: Squared Error Cost Function for Logistic Regression.*

---

# Cost Function for Logistic Regression

## Loss vs. Cost
- **Loss function**: Error for a **single training example**.  
- **Cost function**: Average of the losses across the **entire training set**.  

---

## Logistic Regression Loss Function
- To ensure convexity, we redefine the **loss function** as follows:  

- When **y = 1**:  
  - Loss is **0** if prediction ≈ 1.  
  - Loss grows very large if prediction ≈ 0.  

![Loss Function with y = 1](/Images/C1_W3_LossFunction_Y1.png)  
*Figure: Loss Function when y = 1.*

- When **y = 0**:  
  - Loss is **0** if prediction ≈ 0.  
  - Loss grows very large if prediction ≈ 1.  

![Loss Function with y = 0](/Images/C1_W3_LossFunction_Y0.png)  
*Figure: Loss Function when y = 0.*

➡️ The key idea: The loss uses **two different curves** depending on the label (`y=0` or `y=1`).  
This ensures:
- Loss = 0 when prediction matches the target.  
- Loss increases rapidly when prediction differs from the target.  

---

## Logistic Regression Cost Function
- The overall **cost function** is the average of the above losses.  
- This function is **convex**, which guarantees that **gradient descent converges to the global minimum**.  

![Cost Function for Logistic Regression](/Images/C1_W3_CostFunctionForLogisticRegression.png)  
*Figure: Convex Cost Function for Logistic Regression.*

---

# Simplified Forms

## Simplified Loss Function
- Instead of splitting into two cases (`y=0` or `y=1`), we can combine them into a **single formula**.  

![Simplified Loss Function for Logistic Regression](/Images/C1_W3_SimplifiedLossFunction.png)  
*Figure: Simplified Logistic Regression Loss Function.*

---

## Simplified Cost Function
- Derived from **statistics** using a principle called **Maximum Likelihood Estimation (MLE)**.  
- This is the most commonly used cost function for logistic regression.  

![Simplified Cost Function for Logistic Regression](/Images/C1_W3_SimplifiedCostFunction.png)  
*Figure: Simplified Logistic Regression Cost Function.*
