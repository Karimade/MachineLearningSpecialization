# Train the Model with Gradient Descent

---

## Gradient Descent
- The algorithm used to minimize the cost function is **Gradient Descent**.
- In **Linear Regression**, the initial values of **w** and **b** don’t matter much.
- We typically start with **w = 0** and **b = 0**, and iteratively adjust them to reduce the cost.
- Gradient Descent keeps updating **w** and **b** until the cost function **J(w, b)** reaches a minimum.
- In general, a cost function might have multiple minima (local and global).

![Gradient Descent Visualization](/Images/GradientDescentLocalMinimum.png)
*Figure: Visualizing how Gradient Descent reaches a local minimum.*

---

## Gradient Descent Implementation
```
w = w - α * ∂J/∂w
b = b - α * ∂J/∂b
```
- **α (alpha)** is the **Learning Rate** — a small positive number between 0 and 1.
- The derivative terms (**∂J/∂w**, **∂J/∂b**) indicate the **direction of the steepest slope**.
- The update process is repeated until convergence (when **w** and **b** change very little).
- **Important:** Updates to **w** and **b** should be performed **simultaneously** using their old values, not sequentially.

![Gradient Descent Implementation](/Images/GradientDescentImplementation.png)
*Figure: Visualizing the correct way to update parameters w and b simultaneously.*

---

## Gradient Descent Intuition

### The Derivative Term
- At any point on the cost curve, the **slope (derivative)** indicates which direction to move.
- If the slope is **positive (right side of the curve)** → decrease **w** to move left.
- If the slope is **negative (left side of the curve)** → increase **w** to move right.
- This ensures **w** always moves toward the local minimum.

![Derivative Effect on Steps](/Images/GradientDescentDerivitiveSteps.png)
*Figure: Visualizing how the derivative guides steps toward the minimum.*

### Learning Rate (α)
- **Small α** → Tiny steps → Very slow convergence.
- **Large α** → Overshooting or diverging from the minimum.
- Choosing a suitable **α** is crucial for efficient convergence.

- How does Gradient Descent know when it has reached the minimum?
  - When the **derivative becomes 0**, no further change occurs in **w** or **b**.

![Learning Rate Effect](/Images/GradientDescentAlphaSteps.png)
*Figure: Effect of different learning rates on Gradient Descent steps.*

![Reaching Local Minimum](/Images/ReachingToLocalMinimum.png)
*Figure: Visualizing convergence at the local minimum.*

- Can Gradient Descent converge using a fixed α value?
  - Yes, but the choice of α affects speed and stability.

![Fixed Alpha Effect](/Images/FixedAlphaValueEffect.png)
*Figure: Effect of fixed α value on convergence behavior.*

---

## Gradient Descent for Linear Regression

### Simplifying Derivatives
- For Linear Regression with Squared Error Cost Function, the derivative simplifies to:

![Gradient Descent Formula](/Images/GradientDescentForLinearRegressionFormula.png)
*Figure: Gradient Descent formula for Linear Regression.*

### Computing the Derivatives
- Calculating **∂J/∂w** and **∂J/∂b** involves basic calculus (partial derivatives).
- The factor **1/2m** in the cost function cancels out the 2 that appears during differentiation, simplifying the formula.

![Partial Derivative Process](/Images/DeriveTheGradientDescentDerivative.png)
*Figure: Process of taking partial derivatives for Gradient Descent.*

### Final Simplified Gradient Descent Formula
- After differentiation, the Gradient Descent update formula becomes:

![Simplified Gradient Descent Formula](/Images/SimplifiedGradientDescentFormula.png)
*Figure: Final simplified formula for Gradient Descent parameter updates.*

---

## Local Minima and Global Minimum
- Some cost functions have multiple local minima; Gradient Descent might not always reach the global minimum.
- The outcome depends on initial values of **w** and **b**.
  
![Multiple Local Minima](/Images/GradientDescentGlobalMinimum.png)
*Figure: Example of a cost surface with multiple local minima.*

- However, in **Linear Regression with Squared Error Cost**, the cost surface is **convex** (bowl-shaped).
  - This guarantees there’s only **one global minimum**.
  - Therefore, Gradient Descent will always converge to this point, regardless of the initial parameter values (if α is well-chosen).

---

## Running Gradient Descent
- After running Gradient Descent, we visualize how **w** and **b** evolve towards the optimal values.

![Gradient Descent Results](/Images/VisualizingResultsOfRunningGD.png)
*Figure: Visualization of Gradient Descent results after optimization.*

---

## Batch Gradient Descent
- **Batch Gradient Descent** refers to computing the cost and gradient using **all training examples in each iteration**.
  - Every update step considers the entire dataset.

![Batch Gradient Descent](/Images/BatchGD.png)
*Figure: Concept of Batch Gradient Descent.*

- Other variations:
  - **Stochastic Gradient Descent (SGD):** Uses only **one example per iteration**.
  - **Mini-Batch Gradient Descent:** Uses a **small subset (mini-batch)** of training examples per iteration.

---
