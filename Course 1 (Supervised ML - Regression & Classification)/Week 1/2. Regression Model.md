# Linear Regression and Regression Models

---

## Linear Regression Model
- **Linear Regression** fits a straight line to data to predict continuous numeric values.
- It solves a **Regression Problem**, where the model's output is a continuous number.
- Any model that predicts continuous values is addressing a Regression Problem.
- Linear Regression is one example of a regression model; however, there are other models that solve regression problems as well, such as Polynomial Regression, Decision Trees, etc.

![Linear Regression Model Graph](/Images/LinearRegressionModelGraph.png)
*Figure: Example of fitting a straight line to data using Linear Regression.*

---

## Regression vs Classification Models
| Regression Models                                   | Classification Models                             |
|-----------------------------------------------------|---------------------------------------------------|
| Predict continuous numeric outputs.                 | Predict discrete finite set of outputs (e.g., Cat, Dog). |
| Can output an infinite range of values.             | Outputs belong to predefined categories or labels. |

---

## Training Set Notation
- The dataset used to train the model is called the **Training Set**.
- **Inputs (x)** are also called **features** or **input features**.
- **Outputs (y)** are also known as **targets**, **labels**, or **output values**.
- The number of training examples (rows) is denoted by **m**.

![Training Set Example](/Images/TrainingSet.png)
*Figure: Example structure of a training set with inputs (x) and outputs (y).*

---

## Model Training Process
- The training process involves **feeding the model with the training set** (input features and target outputs).
- A **learning algorithm** processes this data and produces a **function (the model)** that maps an input **x** to a predicted output **y-hat (ŷ)**.

![Training Linear Regression Process](/Images/LinearRegressionTraining.png)
*Figure: Process of training a Linear Regression Model.*

---

## Model Parameters (w and b)
- The linear regression model is defined by the equation:  
  ```
  y = w * x + b
  ```
  where:
  - **w** is the weight (slope of the line)
  - **b** is the bias (y-intercept)
- These are called **model parameters**.
- During training, we adjust **w** and **b** to minimize the prediction error.
- Once the best parameters are found, the model can predict new values, denoted as **y-hat (ŷ)**.

![Model Parameters](/Images/ModelParameters.png)
*Figure: Visualization of how changing w (slope) and b (bias) affects the fitted line.*

---

## Cost Function
- The **Cost Function** is a measure of how well the model's predictions align with the actual data.
- It quantifies the "gap" between predicted values **y-hat (ŷ)** and actual values **y**.
- The objective during training is to **minimize the cost function**.

### Squared Error Cost Function:
The most commonly used cost function for Linear Regression is the **Mean Squared Error (MSE)**, defined as:
```
J(w, b) = (1 / 2m) * Σ (ŷ(i) - y(i))²
```
where:
- **m** is the number of training examples.
- **ŷ(i)** is the predicted value for the i-th example.
- **y(i)** is the actual true value for the i-th example.

![Squared Error Cost Function](/Images/ModelParameters.png)
*Figure: Formula of the Squared Error Cost Function.*

![Cost Function Example Plot](/Images/CostFunctionExamplePlot.png)
*Figure: Visual representation of how different parameter values affect the cost function.*

- The role of the training process is to find the optimal values of **w** and **b** that minimize the cost function **J(w, b)**.
